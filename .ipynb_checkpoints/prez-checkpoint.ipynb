{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "- Hi, I'm Adam.\n",
    "    - Super brief bio\n",
    "        - Links\n",
    "    - Numerous RL projects in the past.\n",
    "        - Linked to on my repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyone in this room can be conceptualized as a series of records that have, do, or will exist.\n",
    "- We begin with a birth certificate ...\n",
    "- ... and end with a death certificate.\n",
    "- In between, there will be medical records, school records, marriage records, bank records, arrest records, etc.\n",
    "\n",
    "Imagine what we could learn about ourselves by integrating all of that information, from all of those different sources, into one single, cohesive story."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick example\n",
    "\n",
    "- Public health example: Joining hospital records to birth certificates.\n",
    "    - What problems would occur?\n",
    "        1. People change\n",
    "            - Names, addresses, ...\n",
    "        2. People make mistakes\n",
    "            - Typos, spelling errors, nicknames, abbreviations, ...\n",
    "        3. People lie\n",
    "            - Age, weight, neighborhood, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional example areas\n",
    "Data matching is not new -- well before computers, we needed to match records belonging to the same individual.\n",
    "- National census\n",
    "    - Governments around the world rely on census data to allocate resources appropriately.\n",
    "    - RL plays an important role in improving the quality and accuracy of census data.\n",
    "    - The U.S. Census Bureau has played a major role in the development of RL techniques for several decades.\n",
    "\n",
    "<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/85/Seal_of_the_United_States_Census_Bureau.svg/200px-Seal_of_the_United_States_Census_Bureau.svg.png\" alt=\"Census Bureau seal\" height=\"140\" width=\"140\">    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Medicine and public health \n",
    "    - (historically referred to as _medical_ record linkage)\n",
    "    - Simply consider all of the doctors, hospitals, insurance companies, and pharmacies you've interacted with and it becomes obvious why medical records are another major RL application area.\n",
    "    - In addition, _longitudinally-matched records_ can provide novel insights into health outcomes, as in the example given previously.\n",
    "    \n",
    "<p><img width=\"256\" alt=\"Seattle physician with patient 1999\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/45/Seattle_physician_with_patient_1999.jpg/256px-Seattle_physician_with_patient_1999.jpg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Customer records\n",
    "    - In order to effectively target their customers, businesses need to minimize the redundancy that tends to occur as a result of changes in name, address, etc.\n",
    "    - This requires businesses to periodically remove redundant records, in order to maintain an accurate record of their customer base (often a main source of revenue) and reach those customers effectively.\n",
    "    \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/7/78/DB-database-icon.png\" alt=\"DB-database-icon.png\" width=\"200\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Genealogy\n",
    "    - Given that more than 10% of men and women were named 'John' and 'Mary' in nineteenth century England, it becomes obvious why RL is an invaluable tool for genealogical databases, some of which are now a billion-dollar industry.\n",
    "    - [LDS have spent a lot of $$$ and published a few papers on RL]\n",
    "    \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a8/1900_census_Kershaw_Lindauer.gif\" alt=\"1900 census Kershaw Lindauer.gif\" height=\"480\" width=\"456\">\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: right\">By 1900 US Census, Public Domain, <a href=\"https://commons.wikimedia.org/w/index.php?curid=11768459\">Link</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do I care?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As a society, we are producing more data than ever before. In order to make use of it, we need intelligent solutions to integrate data from disparate sources.\n",
    "- Such tools play an important role in both data mining _and_ data warehousing -- using RL, we can not only improve the quality (and statistical power) of our data, but also reveal relationships not contained within any single database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "[Make into actual T.O.C. (with links)]\n",
    "\n",
    "## Challenges\n",
    "- First, I want to make you aware of the specific, unique challenges involved with RL, so you can better appreciate why the approach includes the steps it does.\n",
    "\n",
    "### Missing unique identifiers\n",
    "    - Records on people or businesses is often very 'messy' with inconsistent formatting from one database to the next.\n",
    "    - This makes finding unique matches unlikely. Instead, for each record there is a handful of plausible matches, each matching to a varying degree.\n",
    "### Computation complexity\n",
    "    - Because any time you're working with Cartesian products you are working with a quadratic time complexity ($0(n^2)$), intelligent sub-setting of your data (via 'blocking') becomes essential.\n",
    "### Lack of training data\n",
    "    - Due to the expense of hand-labeling training data, we often need to use an unsupervised approach to RL.\n",
    "### Privacy\n",
    "    - In spite of our attempts to keep nice, complete databases, people still demand privacy (go figure).\n",
    "    - As a result, fields that include identifying information are often removed or encrypted, adding to the challenge of our task.\n",
    "    \n",
    "\n",
    "## Classic record linkage approach\n",
    "- Second, I want to give you a snapshot of the classic record linkage approach which will include:\n",
    "\n",
    "### Pre-processing\n",
    "    - Normalization of undesired variation\n",
    "    - Ensuring consistent formatting\n",
    "### Indexing (blocking)\n",
    "    - To reduce the complexity of the task by beginning with smaller subsets of data that are very likely to contain matching records.\n",
    "### Comparison and classification\n",
    "    - Classification of each record pair into 'matches' and 'non-matches' can be performed using either a deterministic (rule-based) or probabilistic (model-based) approach.\n",
    "    - In the probabilistic approach, comparisons between pairs are broken down into feature vectors summarizing their agreement along multiple dimensions, both simple (e.g. name, address, D.O.B.) and complex (e.g. distance between addresses).\n",
    "    - With those feature vectors, we can apply any one of hundreds of different classification models to attempt to predict whether those records are 'matches' or not.\n",
    "### Evaluation\n",
    "    - Evaluation involves determining how successful the linkage was, in terms of correctly identified pairs.\n",
    "    - This can be complicated by the class imbalance between 'match' vs 'non-match' samples.\n",
    "    \n",
    "## Beyond (rename)\n",
    "- Finally, we will cover some advanced topic in RL, including the following:\n",
    "### HMM for pre-processing(?)\n",
    "### Complex features\n",
    "#### NLP\n",
    "### Neural networks, etc.\n",
    "#### Compare performance of multiple classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In all of these case, the challenge that we have to overcome is missing a unique identifier for the entities we are matching.\n",
    "- For example, if we had perfectly accurate social security numbers for each record, the task is reduced to a straight-forward join of two databases.\n",
    "- This is often not the case for multiple reasons:\n",
    "    1. accurate record keeping is hard\n",
    "    2. privacy is usually a concern (in some countries use of such identifiers is illegal).\n",
    "- As such, in order to match records across databases, we must use common attributes shared by both databases.\n",
    "    - e.g. Name, address, phone number, age\n",
    "- The quality of data points such as these are notoriously low for reasons described earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation complexity\n",
    "- As a naive approach, one might try comparing each record in one database, to each record in the other, to determine if each pair under consideration might be a match.\n",
    "- The computational complexity of such an approach, however, grows quadratically ($O(NÂ²)$) with the size of the smaller database.\n",
    "- As we'll see, some nice tricks exist to reduce the size of the problem substantially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lack of training labels\n",
    "- In the typical (supervised) machine learning approach, labeled training data is used as feedback by a statistical model during the process of training. \n",
    "- In some cases, the there is no training data that tells us if two records correspond to the same individual or not.\n",
    "- This can make the evaluation of the model's matches especially challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy\n",
    "- Given that these records often contain sensitive personal information (such as medical/employment records), special attention must be paid to preserving this privacy via _'de-identification'_.\n",
    "- This is especially important for academic or medical researchers using HIPAA-protected datasets for research use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic record linkage approach\n",
    "I will use 'record linkage' to refer to both the matching of records across two (or more) databases.\n",
    "- This can also include the special case of _'de-duplication'_, which simply involves using the same approach* to find duplicate records in _the same_ database.\n",
    "\n",
    "<div style=\"text-align: right\">*De-duplication can sometimes involve matching more than 2 records within a database.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most commonly, each record refers to a real-live person*.\n",
    "- Customers in a business database\n",
    "- Constituents in a government database\n",
    "- Patients in a hospital database\n",
    "<div style=\"text-align: right\">*Sometimes the entity to be matched is a business, or some other object.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/rl_pipeline_figure.png\" alt=\"RL pipeline figure\" width=\"456\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pre-processing\n",
    "\n",
    "<img src=\"assets/record_examples.png\" alt=\"Example records\" width=\"700\">\n",
    "\n",
    "Records from different databases often vary wildly in their formatting conventions.\n",
    "- As a result, it falls to use to ensure that the data we want to compare has been properly cleaned and standardized.\n",
    "- Any inconsistencies _must_ be resolved for successful linkage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the potential problems that may need to be addressed during pre-processing are too numerous to list -- we can refine the process into three major steps:\n",
    "1. Removing undesired characters/words\n",
    "    - Non-alphanumeric characters\n",
    "    - In some cases, removing irrelevant words (_stop words_) is useful.\n",
    "2. Standardize abbreviations and correct typos\n",
    "    - Use hash mapping to reduce the variation of equivalent values.\n",
    "3. Parsing input to create new variables (feature engineering)\n",
    "    - As we'll see, parsing our raw data into it's component elements allows us to model each of them individually, often resulting in a better performing model _and_ a greater ability to make inferences about which variables are most important during classification.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Regardless of the specific pre-processing steps that you perform - <b>It is important to not over-write the original, raw data!</b>\n",
    "> - Otherwise, there is no guarantee that it can be recovered after being transformed.\n",
    "> - Later, different pre-processing may be desired.\n",
    "> Ideally, new copies of the data are created after each major transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ideally, after pre-processing, those same records would look something like this below.\n",
    "\n",
    "<img src=\"assets/record_clean.png\" alt=\"Example records\" width=\"800\">\n",
    "\n",
    "- Our standardized data now contains all attributes from both databases.\n",
    "- Content has been standardized.\n",
    "- Contradicting fields have been corrected\n",
    "- Abbreviations have been expanded\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Indexing\n",
    "Now, we are ready to compare our records to look for a match.\n",
    "- But, if we are dealing with typical databases containing, say, a million or more records -- clearly we are not capable of comparing one trillion record pairs in a reasonable* time span.\n",
    "\n",
    "<div style=\"text-align: right\">*Ideally, we're talking minutes to hours, not days or weeks.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like any good algorithm designer, though, we can start to think about where we can save ourselves from doing work.\n",
    "- The vast majority of record comparisons will be non-matches.\n",
    "- Especially so for records that are dis-similar along particular dimensions.\n",
    "> For example, while matching record pairs may sometimes contain different phone numbers, they will almost never contain different genders. \n",
    "> - As a result, we can reduce the complexity of our algorithm substantially by simply comparing only records matching on gender.\n",
    ">\n",
    "> <img src=\"assets/blocking.png\" alt=\"Blocking example\" width=\"800\">\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Blocking</i></b>, is a similar approach to indexing, which relies on a small number of such features to reduce the number of comparisons.\n",
    "- 'zip code' and 'phonetically-encoded surname' are two such examples.\n",
    "> <b>Warning!</b>: This approach does, however, sometimes miss certain matches that may, for example, contain a typo in one of the <i>blocking keys</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
