{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       ".rendered_html {\n",
       "  font-size:1.00em;\n",
       "}\n",
       ".rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
       "     font-size: 100%;\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "\n",
    ".rendered_html {\n",
    "  font-size:1.00em;\n",
    "}\n",
    ".rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
    "     font-size: 100%;\n",
    "}\n",
    "\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Data deduplication with Python</h1></center>\n",
    "<center><h2><font color='grey'>Applying modern tools to the 'classic' linkage approach.</font></h2></center>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Today, I'm going to talk about <b>[<font color='blue'>record linkage</font>](https://en.wikipedia.org/wiki/Record_linkage) (RL)</b> - <i>the process of matching multiple records that correspond to the same entity.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> #### <font color='orange'>Jupyter Notebook:</font> [github.com/meccaLeccaHi/RL_pybay2020](https://github.com/meccaLeccaHi/RL_pybay2020)\n",
    "- I'll paste the link in the group chat -- <b>`clone` it</b> and <i>follow along!</i>\n",
    ">\n",
    "> #### <font color='purple'>Slack:</font> [#talk-data-deduplication-with-python](https://pybay2020.slack.com/archives/C018VEY8DKK)\n",
    "> - Feel free to <b>ask questions</b> after the talk!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img align=\"right\" src=\"https://i.imgur.com/wr0gqAH.png\" width=\"300\" height=\"300\">\n",
    "\n",
    "<center><h1><font color='green'>Hi!</font></h1></center>\n",
    "\n",
    "- I'm [Adam](https://www.adam-p-jones.com/).\n",
    "    - I'm a [data scientist](https://www.linkedin.com/in/adam-p-jones/) with an interest in complex problems in healthcare.\n",
    "    - Formerly a [Neuroscientist](https://pubmed.ncbi.nlm.nih.gov/?term=adam+p+jones) at the [NIH](https://www.nih.gov/).      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Past projects:\n",
    "- [Maternal data linkage](https://github.com/meccaLeccaHi/record_linkage) (2019) -- Critical Juncture\n",
    "    - Effort to improve understanding of perinatal health outcomes in California.\n",
    "- [Testing the impact of health workers in Mali](https://www.datakind.org/blog/tracking-patients-across-years-with-record-linkage) (2020) -- DataKind SF\n",
    "    - Probabilistic record linkage across consecutive census survey years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"motivation\"></a>\n",
    "<font color='grey'><h1>Motivation</h1></font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Everyone listening to me be conceptualized as a <i>series of records</i> that have, do, or will exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We all begin with a <b>birth certificate</b>. \n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"assets/birth_certs.png\" width=\"1400\" alt=\"birth certificates\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We all end with a <b>death certificate</b>.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"assets/death_certs.png\" width=\"1400\" alt=\"death certificates\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In between, there will be <b>medical records, school records, marriage records, bank records, arrest records,</b> etc.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"assets/in_between.png\" width=\"1200\" alt=\"records examples\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> <b><i>Imagine</i></b> what we could learn about ourselves by integrating all of that information, from all of those different sources, into one single, cohesive story..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> ... with consistent formatting, and no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And, if data were perfectly clean, this would all reduce to a simple [`JOIN`](https://en.wikipedia.org/wiki/Join_(SQL)) operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### But it's <font color='#CC0033'><i>NOT</i></font>.\n",
    "- This is why we need techniques to integrate inconsistent data.\n",
    "- Those techniques will be the focus of this talk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quick example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<b>Public health problem</b>: Joining hospital records to death certificates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> <i>What problems are likely to occur?</i>\n",
    "> 1. People change\n",
    ">     - Names, addresses, ...\n",
    "> 2. People make mistakes\n",
    ">     - Typos, spelling errors, nicknames, abbreviations, ...\n",
    "> 3. People lie\n",
    ">     - Age, weight, neighborhood, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"application\"></a>\n",
    "## History and Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Data matching is <i>not</i> new. \n",
    "- Well before computers, we needed to match records belonging to the same individual.\n",
    "- It is not known when, exactly, record linkage first began.\n",
    "- We <i>do</i> know that record-keeping, itself, goes all the way back to the beginning of written-language.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/3/3d/Rembrandt_-_Moses_with_the_Ten_Commandments_-_Google_Art_Project.jpg\" alt=\"Early databases\" width=\"300\">\n",
    "</center>\n",
    "\n",
    "<center><i>Possible first attempt to join two databases (Rembrandt, 1659).</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img align=\"right\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7f/Forward_font_awesome.svg/1024px-Forward_font_awesome.svg.png\" width=\"100\">\n",
    "\n",
    "> - But, in the interest of time, we'll skip ahead a few millenia..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### National census\n",
    "- Governments around the world rely on census data to allocate resources appropriately.\n",
    "- RL plays an important role in improving the quality and accuracy of census data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/85/Seal_of_the_United_States_Census_Bureau.svg/200px-Seal_of_the_United_States_Census_Bureau.svg.png\" alt=\"Census Bureau seal\" width=\"250\">\n",
    "</center>  \n",
    "\n",
    "> The [U.S. Census Bureau](https://www.census.gov/) has played a major role in the development of RL techniques for several decades -- developed several popular algorithms widely used in RL today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Medicine and public health \n",
    "- Simply consider all of the doctors, hospitals, insurance companies, and pharmacies you've interacted with and it becomes obvious why medical records are one of the largest RL application areas.\n",
    "- In addition, <b>longitudinal-matching</b> of records can provide novel insights into health outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/45/Seattle_physician_with_patient_1999.jpg/256px-Seattle_physician_with_patient_1999.jpg\" width=\"325\" alt=\"Seattle physician with patient 1999\" >\n",
    "</center>\n",
    "\n",
    "> In both the U.K. and Australia longitudinal data matching has been used successfully to examine health outcomes for hundreds of thousands of individuals using anonymized medical records.\n",
    "> - This has lead to <b>valuable discoveries</b> regarding disease, mortality, migration, and socio-economic outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/en/7/78/DB-database-icon.png\" alt=\"database icon\" width=\"250\">\n",
    "</center>\n",
    "\n",
    "#### Customer records\n",
    "- In order to effectively target their customers, businesses need to minimize the redundancy that tends to occur as a result of changes in name, address, etc.\n",
    "- This requires the <b>periodic removal of redundant records</b>, in order to maintain an accurate record of their customer base (often a main source of revenue) and reach those customers effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Genealogy\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/900-158_Ahnentafel_Herzog_Ludwig.jpg/1920px-900-158_Ahnentafel_Herzog_Ludwig.jpg\" alt=\"family tree\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "> - <b><i>Interesting Fact:</i></b> More than 10% of men and women were named 'John' and 'Mary', respectively, in 19th-century England."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Issues like these make tools such as RL critical for genealogical databases, which are now a billion-dollar industry.\n",
    "- [FamilySearch](https://en.wikipedia.org/wiki/FamilySearch) (i.e. Church of LDS) have invested heavily in RL and have published several technical reports on the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img align=\"right\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b8/Man-scratching-head.gif/247px-Man-scratching-head.gif\" alt=\"head scratch\" width=\"100\">\n",
    "\n",
    "<a id=\"why\"></a>\n",
    "## Why do I care?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As a society, we are producing more data than ever before. \n",
    "- In order to make use of it, we need intelligent solutions to integrate data from disparate sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "RL lets you <b><i>do more</i></b> with your data.\n",
    "- RL can be used to enhance dimensions including <b>time, breadth,</b> or <b>depth</b> (of detail). \n",
    "- As a result, these tools play an important role in both <b>data warehousing</b> <i>and</i> <b>data mining</b>.\n",
    "    - We can not only improve the <i>quality</i> (and statistical power) of our data, \n",
    "    - But also <i>reveal unknown relationships</i> not contained within any single database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"objectives\"></a>\n",
    "### Objectives\n",
    "<b>1.</b> Make you aware of the specific, unique challenges involved with RL.\n",
    "\n",
    "<b>2.</b> Give you a snapshot of the (neo-)classic record linkage approach.\n",
    "\n",
    "<b>3.</b> Go through a demo together in which we compare the performance of a variety of classification models that we'll use for RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- [Motivation](#motivation)\n",
    "    - [Quick example](#example)\n",
    "    - [History and Application](#application)\n",
    "    - [Why do I care?](#why)\n",
    "        - [Objectives](#objectives)\n",
    "- [Challenges](#challenges)\n",
    "    - [Missing unique identifiers](#unique)\n",
    "    - [Computational complexity](#complexity)\n",
    "    - [Lack of training labels](#labels)\n",
    "    - [Privacy](#privacy)\n",
    "- [Classic record linkage](#classic)\n",
    "    - [Pre-processing](#pre)\n",
    "        - [Handling missing values & outliers](#handling)\n",
    "        - [Segmentation](#segmentation)\n",
    "        - [Phonetic encoding](#phonetic)\n",
    "    - [Indexing (blocking)](#blocking)\n",
    "        - [Defining blocking keys](#keys)\n",
    "    - [Comparison](#comparison)\n",
    "        - [Strings](#strings)\n",
    "        - [Numbers](#numbers)\n",
    "        - [Time](#time)\n",
    "        - [Space](#space)\n",
    "    - [Classification](#classification)\n",
    "        - [Assignment](#assignment)\n",
    "    - [Evaluation](#evaluation)\n",
    "- [Demo](#demo)\n",
    "    - [Comparing classifiers](#comparing)\n",
    "        - [Supervised classifiers](#supervised)\n",
    "        - [Unsupervised classifiers](#unsupervised)\n",
    "- [Conclusions](#conclusions)\n",
    "    - [Resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<a id=\"challenges\"></a>\n",
    "<font color='grey'><h1>Challenges</h1></font>\n",
    "\n",
    "---\n",
    "\n",
    "i.e. <i>'Why can't I just `JOIN`?'</i>\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/bear.png\" alt=\"Why??\" width=\"200\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"unique\"></a>\n",
    "## Missing unique identifiers\n",
    "\n",
    "In all of these cases, the challenge that we have to overcome is missing a unique identifier for the entities we are matching.\n",
    "- <i>For example</i>, if we had perfectly accurate social security numbers for each record, the task would be reduced to a straight-forward join of two databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is <i>often</i> not the case for multiple reasons:\n",
    "1. Accurate record keeping is <i>hard</i>.\n",
    "    - Mistakes and inaccuracies lead to undesired variation.\n",
    "    \n",
    "2. Privacy is usually a concern \n",
    "    - In some countries use of such identifiers is illegal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As such, in order to match records across databases, we must use common attributes shared by both databases.\n",
    "- e.g. Name, address, phone number, age.\n",
    "- But, the quality of data points such as these are <i>notoriously low</i> for reasons described earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/User-privacy-icon.svg/500px-User-privacy-icon.svg.png\" alt=\"privacy icon\" width=\"100\">\n",
    "</center>\n",
    "\n",
    "## Privacy\n",
    "<a id=\"privacy\"></a>\n",
    "\n",
    "In the event that these records contain sensitive <b>personal information</b> (such as medical/employment info), special attention must be paid to preserving this privacy via <i>'de-identification'</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This means that identifying information are often removed or encrypted before we get them, adding to the challenge of our task.\n",
    "- This is especially important for academic or medical researchers using [HIPAA](https://en.wikipedia.org/wiki/Health_Insurance_Portability_and_Accountability_Act)-protected datasets for research use.\n",
    "    - Accountability and oversight are usually much greater in those cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Noun_project_network_icon_1365244_cc.svg/574px-Noun_project_network_icon_1365244_cc.svg.png\" alt=\"complexity icon\" width=\"150\">\n",
    "</center>\n",
    "\n",
    "\n",
    "## Computational complexity\n",
    "<a id=\"complexity\"></a>\n",
    "\n",
    "As a naive approach, one might try comparing each record in one database, to each record in the other, to determine if each pair under consideration might be a match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The computational complexity - $O(N²)$ \n",
    "    - The time it takes to use this approach <b>grows <i>quadratically</i></b> with the size of the smaller database.\n",
    "- This is fine for very small databases (hundreds of records).\n",
    "    - But, it becomes infeasible when dealing with today's typical databases (>hundreds of thousands of records).\n",
    "- As we'll see, some nice tricks exist to <i>substantially</i> reduce the size of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"labels\"></a>\n",
    "## Lack of training labels\n",
    "In the typical (supervised) machine learning approach, labeled training data is used as feedback by a statistical model during the process of training. \n",
    "- In some cases, the there is <i>no training data</i> that tells us if two records correspond to the same individual or not.\n",
    "    - i.e. <i>'unsupervised'</i>\n",
    "- This can make the evaluation of the matches produced by a model especially challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, we'll describe some <b>solutions</b> to several of these problems.\n",
    "- I'm going to introduce you to the <b><i>classic record linkage approach</i></b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"classic\"></a>\n",
    "<font color='grey'><h1>'Classic' record linkage</h1></font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Although, sometimes the entity to be matched is a business, product, or some other object...\n",
    "\n",
    "- Most commonly, the records we seek to link refer to a real, live <b>person</b> (shown here).\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/6/68/Akha_cropped_hires.JPG\" alt=\"People\" width=\"275\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<b>Examples:</b>\n",
    "- Customers in a business database\n",
    "- Constituents in a government database\n",
    "- Patients in a hospital database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In general, I will use <i>'record linkage'</i> to refer to the matching of records between databases.\n",
    "- But, this includes the special case of <b><i>'de-duplication'</i></b>, which simply involves using the same approach to find duplicate records in a <i>single</i> database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"assets/rl_pipeline_figure.png\" alt=\"RL pipeline figure\" width=\"750\">\n",
    "    <i>Typical RL pipeline.</i>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> #### Let's break it down -- starting with <font color='#CC3333 '><i><b>pre-processing</b></i></font>..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"pre\"></a>\n",
    "<font color='#CC3333 '><h2>Pre-processing</h2></font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Records from different databases often vary wildly in their formatting conventions.\n",
    "- Notice the <i>extremely-common</i> inconsistencies below...\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"assets/record_example_A.png\" alt=\"Example record A\" width=\"950\">\n",
    "</center>\n",
    "<center>\n",
    "    <img src=\"assets/record_example_B.png\" alt=\"Example record B\" width=\"950\">\n",
    "</center>\n",
    "\n",
    "As a result, it falls to us to ensure that the data we want to compare has been properly <i>cleaned</i> and <i>standardized</i>.\n",
    "- Any inconsistencies <i>must</i> be resolved for successful linkage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Although the potential problems that may need to be addressed during pre-processing are too numerous to list -- we can refine the process into <b>three major steps</b>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<b>1. Removing undesired characters/words</b>\n",
    "- Non-alphanumeric characters\n",
    "- In some cases, removing irrelevant words (<i>stop words</i>) is useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<b>2. Standardize abbreviations and correct typpos</b>\n",
    "- Use hash mapping to reduce the variation of equivalent values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<b>3. Parsing input to create new variables (feature engineering)</b>\n",
    "- As we'll see, parsing our raw data into it's component elements allows us to model each of them individually, often resulting in a better performing model <i>and</i> a greater ability to make inferences about which variables are most important during classification.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br>\n",
    "<img align=\"right\" src=\"assets/warning.png\" width=\"200\">\n",
    "\n",
    "> Regardless of the specific pre-processing steps that you perform - <b>don't over-write the original data!</b>\n",
    "> - Otherwise, there is no guarantee that it can be recovered after being transformed.\n",
    "> - Later, different pre-processing may be desired.\n",
    "> - Ideally, new copies of the data are created after each major transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"handling\"></a>\n",
    "### Handling missing values & outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Missing values\n",
    "- The <b>removal of rows</b> in RL can result in fewer correctly-linked record pairs, so this option <i>is usually avoided</i>.\n",
    "    - The exception being rows with so little information they are unlikely to be matched anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The <b>removal of columns</b> can also be detrimental to the accuracy of our linkage.\n",
    "    - Even if there are many records missing a value, the records without missing values will still be able to use that information for matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Outliers\n",
    "Smoothing or removing noisy values can also hurt the accuracy of our linkage, so <i>it is usually not applied</i>.\n",
    "- Outliers can still contain useful information.\n",
    "    - For example, an age of '250' could actually be a typo of '25', which would actually be fairly close in string similarity (e.g. edit distance). \n",
    "    - This information would be lost if smoothing were applied.\n",
    "- Our model won't see them anyway! \n",
    "    - Unusual values usually don't make it beyond the comparison step, so they aren't likely to strongly affect our predictions, anyways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"phonetic\"></a>\n",
    "### Phonetic encoding\n",
    "As part of standardizing our data, we will often need to convert strings into their [phonetic encoding](https://en.wikipedia.org/wiki/Phonetic_algorithm).\n",
    "\n",
    "- This allows us to compare words based on the way they are spoken, rather than the way they are spelled.\n",
    "    - The latter can be more <i>regionally-specific</i>.\n",
    "- Specifically, we will convert a string (e.g. a name) into a coded representation of how it is spoken.\n",
    "    - Example codes: Soundex, Phonex, Fuzzy Soundex\n",
    "- These approaches are linguistic in nature, and so specific to a particular language (usually English)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> After encoding, these de-duplicated features can be useful for <b>'blocking'</b>, which we'll describe in the following section on Indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"segmentation\"></a>\n",
    "### Segmentation\n",
    "As part of feature engineering, we often break down many of our variables (e.g. full name) into several new variables (e.g. title, first name, last name).\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"assets/segmentation.png\" alt=\"Segmentation example\" width=\"1000\">\n",
    "</center>\n",
    "\n",
    "- This is often the most challenging step in pre-processing, because of all the possible ways to parse our data into new features.\n",
    "- In general, this will lead to better matching by making more information available during the comparison step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<b>Rule-based</b> segmentation approaches work well for well-structured fields containing consistent information.\n",
    "- Examples: regex, `if-then` statements, etc.\n",
    "- Although, manual approaches like these require a greater invest of time, they can become quite robust with enough labor (and iterations) invested.\n",
    "    - Unfortunately, they will still always be susceptible to unexpected variations in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<b>Statistical</b> approaches to segmentation work by learning probability distributions regarding how elements in a field should be broken down into its constituent tokens.\n",
    "- This approach entails classification of each element in a field as a particular token.\n",
    "- More robust to changes in data formatting over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The most popular tool for the statistical approach has been the [<b>hidden Markov model</b>](https://en.wikipedia.org/wiki/Hidden_Markov_model).\n",
    "- Notice, obtaining/creating data to train HMM is sometimes as much effort as assigning rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ideally, after pre-processing, those same records would look something like this below.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"assets/record_clean.png\" alt=\"Example records\" width=\"1300\">\n",
    "</center>\n",
    "\n",
    "- Our data now contains all attributes from both databases.\n",
    "- Content has been standardized.\n",
    "- Contradicting fields have been corrected.\n",
    "- Abbreviations have been expanded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<b>After pre-processing</b>, we should <i>at least</i> understand:\n",
    "1. the number of unique values for each feature\n",
    "2. the frequency of those values, and how many missing values each feature contains\n",
    "\n",
    "\n",
    "> We will rely heavily on this information in the next stage: <font color='#CC3333'><b>indexing</b></font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"assets/indexing.png\" alt=\"indexing\" width=\"450\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"blocking\"></a>\n",
    "<font color='#CC3333'><h2>Indexing (blocking)</h2></font>\n",
    "\n",
    "---\n",
    "\n",
    "Now, we are ready to compare our records to look for a match.\n",
    "- But, if we are dealing with typical databases containing, say, a million or more records -- clearly we are not capable of comparing <i>one trillion</i> record pairs in a <i>\"reasonable\"</i>* time span.\n",
    "\n",
    "<br></br>\n",
    "<div style=\"text-align: right\">*Ideally, we're talking minutes to hours, not days or weeks.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"assets/thinker.png\" alt=\"thinker\" width=\"300\">\n",
    "</center>\n",
    "\n",
    "Like any good algorithm designer -- we can <i>think</i> about where we can <b>save ourselves from doing work</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The vast majority of record comparisons will be non-matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Especially so for records that are dis-similar along particular dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So, we should work with smaller subsets of data that are very likely to contain matching records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> <b>For example,</b> while matching record pairs may sometimes contain different 'phone number's, they will almost never contain a different 'gender'. \n",
    "> - As a result, we can reduce the complexity of our algorithm substantially by simply comparing only records matching on 'gender'.\n",
    ">\n",
    "> <center>\n",
    "<img src=\"assets/blocking.png\" alt=\"Blocking example\" width=\"1100\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<b><i>Blocking</i></b>, is a similar approach to indexing, which relies on a small number of such features to reduce the number of comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the previous example, <i>we would only have two blocks</i> (one for 'male' and one for 'female') for each database -- not ideal.\n",
    "- 'zip code' and phonetically-encoded 'surname' are <i>much</i> better candidates for use as <i>blocking keys</i>.\n",
    "- For greater improvements in performance, it is also common to block using <i>multiple</i> variables, in succession."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img align=\"right\" src=\"assets/warning.png\" width=\"200\">\n",
    "\n",
    "> <b>Warning!</b>: This approach does, however, sometimes miss certain matches that may, for example, contain a typo in one of the <i>blocking keys</i>.\n",
    "> - In the example above, the record would not have any chance of being matched in the event that 'gender' changed or was entered incorrectly.\n",
    "> - This highlights the need for careful selection of blocking criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"keys\"></a>\n",
    "### Defining blocking keys\n",
    "\n",
    "Given that the comparison step (covered in the next section), is the most computationally-expensive step in the RL process, proper selection of <b>blocking keys can have a <i>major</i> impact on efficiency</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Data quality/consistency\n",
    "One major consideration must be the quality of the values in the fields selected as blocking keys. \n",
    "- This refers to both their completeness and their frequency distributions.\n",
    "- Ideally, the feature selected as the blocking key will be as complete as possible (few errors or missing values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Number vs. size of blocks\n",
    "The frequency distribution of values in a blocked feature will determine the size of the blocks that are compared.\n",
    "- A <b>small number of large blocks</b> will result in a more comparisons (more computational expense), but are less likely to miss true matches.\n",
    "- However, <b>large number of small blocks</b> will result in less computational expense, but are more likely to miss true matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, blocking by 'gender', as we did in our example earlier, would only result in blocks that were half the size of our original dataset -- not much of an improvement when dealing with quadratic time complexity.\n",
    "- Higher-cardinality variables, such as 'zip code', often result in blocks of appropriate size.\n",
    "- Using a variable with too high of cardinality will result in a lower likelihood of our blocks containing true matches.\n",
    "- Fortunately, blocking keys can be optimized just like any other hyper-parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Parallel processing\n",
    "Blocking is also an opportunity to leverage parallel processing approaches to decrease processing time, when that is applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> After <b>indexing</b> is complete, we are ready to begin the <font color='#CC3333'><i><b>comparison</b></i></font> step.\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/comparison.png\" alt=\"comparison\" width=\"450\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"comparison\"></a>\n",
    "<font color='#CC3333'><h2>Comparison</h2></font>\n",
    "\n",
    "---\n",
    "\n",
    "Next, the similarity between our candidate pairs is calculated by comparing several record attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Classification of each record pair into \"matches\" and \"non-matches\" can be performed using either a <b><i>deterministic</i></b> (rule-based) or <b><i>probabilistic</i></b> (model-based) approach.\n",
    "    - We'll focus on using a probabilistic approach, in which comparisons between pairs are broken down into feature vectors summarizing their agreement along multiple dimensions.\n",
    "- With the <b><i>feature vector</i></b> generated during comparison, we can apply any one of hundreds of different classification models.\n",
    "    - Models are trained to predict whether records are \"matches\" or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Comparisons can range from simple numeric comparisons, like the difference between ages of each record, to more complex comparison functions, like 'fuzzy' string matching or distances between addresses. \n",
    "- A few examples of such comparisons are shown below.\n",
    "<center>\n",
    "<img src=\"assets/record_comparison.png\" alt=\"Record comparison\" width=\"650\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The result is a <b>comparison vector</b> for each record pair.\n",
    "- We used approximate comparisons for strings, [edit distance](https://en.wikipedia.org/wiki/Edit_distance) for numbers, and equivalence for Booleans.\n",
    "- This <i>feature vector</i> is what we use for classification.\n",
    "- Using those feature vectors, we can apply any available classification models (sklearn, statsmodels, tensorflow, etc.) to attempt to predict whether those records are \"matches\" or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"strings\"></a>\n",
    "### Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "#### String comparisons\n",
    "Strings are usually the most likely type to contain errors and typos, so rather than use exact matching, most RL employ comparison functions that return some indication of similarity on a continuum.\n",
    "- [Edit distance](https://en.wikipedia.org/wiki/Edit_distance) (i.e. [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance)): Smallest number of single character insertions, deletions, and substitutions that are required to convert one string into the other.\n",
    "- [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance): Measures the minimum number of substitutions required to change one string into the other.\n",
    "- [Jaro-Winkler distance](https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance): Developed at the U.S. Census Bureau -- it counts the number of equivalent characters within half the length of the longer string, and the number of transposes in the sets of common strings.\n",
    "\n",
    "| Distance | Example   |\n",
    "|------|------|\n",
    "| Levenshtein | \"kitten\" and \"sitting\" is 3 |\n",
    "| Hamming | \"karolin\" and \"kathrin\" is 3 |\n",
    "| Jaro–Winkler | \"martha\",\"marhta\" is 0.944 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# textdistance - https://pypi.org/project/textdistance/\n",
    "# install via: $ pip install textdistance\n",
    "\n",
    "import textdistance\n",
    "\n",
    "print('Levenshtein:', textdistance.levenshtein('banana', 'bahama') )\n",
    "\n",
    "print('Hamming:', textdistance.hamming('banana', 'bahama') )\n",
    "\n",
    "print('Jaro:', textdistance.jaro_winkler('banana', 'bahama') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"names\"></a>\n",
    "#### Names\n",
    "...play a big role in RL.\n",
    "- Since names are part of language, we know there are usually general rules, but those rules have lots of variability.\n",
    "- e.g. 'Robert', 'Bob', 'Bobby', 'Rob', 'Roberto'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<i>Culture</i>\n",
    "- First, middle, family name is an Anglo-Saxon convention -- not representative of <i>most</i> of the world.\n",
    "- Compound names are common in many other cultures (e.g. 'Santiago Ramón y Cajal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How names change, or stay the same, can also be very <i>culturally-specific</i>.\n",
    "- In some places, first and last name are sometimes used in the reverse order. \n",
    "- For this reason, it also a good idea to include cross comparisons.\n",
    "    - For example, including a comparison of first name with last name can catch instances of their reversal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"numbers\"></a>\n",
    "### Numbers\n",
    "In addition to strings, RL also sometimes includes numerical comparisons.\n",
    "- For example, comparing records including age or financial data such as income or expenses.\n",
    "- As with strings, we would like to be able to calculate an approximate similarity between two values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<i>Typically</i>, we accomplish this by thresholding their difference (or percentage) relative to some cutoff ($d_{max}$).\n",
    "- We linearly extrapolate anything below that to between 1.0 (perfect similarity) and 0.0 (total dissimilarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Maximum absolute difference ($d$)\n",
    "$$\n",
    "d = |n_1-n_2|\\\\\n",
    "sim_{absolute\\_difference} = \\left\\{\n",
    "    \\begin{array}{l}\n",
    "      1.0-\\frac{d}{d_{max}} \\hspace{3mm} if d < d_{max} , \\\\\n",
    "      0.0  \\hspace{22mm}  else\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Maximum percentage difference ($pd$)\n",
    "\n",
    "$$\n",
    "pd = \\frac{|n_1-n_2|}{max(|n_1|,|n_2|)}*100\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "sim_{percentage\\_difference} = \\left\\{\n",
    "    \\begin{array}{l}\n",
    "      1.0-(\\frac{pd}{pd_{max}}) \\hspace{3mm} if pd < pd_{max} , \\\\\n",
    "      0.0  \\hspace{22mm}  else\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"time\"></a>\n",
    "### Time\n",
    "\n",
    "We can treat time data ('date's, 'age's, 'time' comparisons, etc.) as a special case of numerical data and use a similar approach.\n",
    "- For example, the we can calculate an percentage difference for 'age' ($apd$):\n",
    "\n",
    "$$\n",
    "apd = \\frac{|d_1-d_2|}{max(d_1,d_2)}*100\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ... and then handle it just as we did above:\n",
    "\n",
    "$$ \n",
    "sim_{apd} = \\left\\{\n",
    "    \\begin{array}{l}\n",
    "      1.0-(\\frac{apd}{apd_{max}}) \\hspace{3mm} if apd < apd_{max} , \\\\\n",
    "      0.0  \\hspace{22mm}  else\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/Rotating_earth_%28huge%29.gif/600px-Rotating_earth_%28huge%29.gif\" alt=\"space\" width=\"300\">\n",
    "</center>\n",
    "\n",
    "\n",
    "<a id=\"space\"></a>\n",
    "### Space\n",
    "As geocoding resources are becoming increasingly popular and available, it's worth mentioning that they can be employed in RL, as well.\n",
    "- Rather than simply comparing the 'address', we can actually calculate the distance (latitude and longitude) between points on Earth.\n",
    "- The major bottleneck to this process, is the <i>quality of address information</i> that is available to use for geocoding.\n",
    "- If any address details are missing or erroneous, then the location data provided may be imprecise or incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> With our <b>comparisons</b> complete, we can begin the <font color='#CC3333'><b><i>classification</i></b></font> step.\n",
    "\n",
    "<center>\n",
    "    <img src=\"assets/classification.png\" alt=\"classification\" width=\"450\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"classification\"></a>\n",
    "<h2><font color='#CC3333'>Classification</font></h2>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using our comparison vector, we can now use a [<i>classification model</i>](https://en.wikipedia.org/wiki/Statistical_classification) to classify each record pair as either a <b>\"match\"</b> or <b>\"non-match\"</b>.\n",
    "<center>\n",
    "    <img src=\"assets/model.png\" alt=\"Modeling example\" width=\"600\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> We'll skip most the details of the standard modeling approach, as they are well-described.\n",
    "> - [This resource](http://faculty.marshall.usc.edu/gareth-james/ISL/) provides an excellent introduction to this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In recent years, a variety of machine learning classifiers have been found useful in record linkage applications. \n",
    "- It has been recognized that the algorithm classically associated with record linkage -- i.e. the 'probabilistic' approach -- is equivalent to the [Naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\n",
    "    - As a result, it suffers from the same (typically un-true) assumption of <i>feature-independence</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- <i>Higher</i> accuracy can often be achieved using other machine learning techniques, such as a single-layer [perceptron](https://en.wikipedia.org/wiki/Perceptron), which <b><i>does not</i></b> rely on the independence assumption\\*.\n",
    "<div style=\"text-align: right\">*See <a href=\"http://axon.cs.byu.edu/~randy/pubs/wilson.ijcnn2011.beyondprl.pdf\" ><i>Wilson (2011)</i></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <i>As always</i>, the model you choose should depend on the complexity of the data.\n",
    "- The greater the number of <i>complex field comparisons</i>, with many possible values, the more complex the comparison vector.\n",
    "- Naturally, a more complex model will be more able to utilize the information in those complex fields, giving it the potential to discriminate more accurately.\n",
    "    - <i>But</i>, complex models will also have a greater tendency to become <b><i>overfit</i></b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> #### Almost there!\n",
    "> - With the predictions of our classifiers in hand, we are now ready to proceed to the <b>assignment</b> step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"assignment\"></a>\n",
    "### Assignment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given that this operation is performed on each record pair, <i>independently</i> of all other pairs, a given record may classify as a match for more than one record.\n",
    "- The simplest way (a greedy algorithm), would result in some assignments being not optimal due to [transitive closure](https://en.wikipedia.org/wiki/Transitive_closure).\n",
    "- It's a classic [<b><i>assignment problem</i></b>](https://en.wikipedia.org/wiki/Assignment_problem)!\n",
    "- It's an often-overlooked step in the RL process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<b>For example,</b> imagine you got the following output from your classifier.\n",
    "- Records $A$ and $B$ have a match weight of $42.21$.\n",
    "- But record $A$ has a weight of $39.01$ with $C$, and records $B$ and $C$ have a weight of $44.98$.\n",
    "- Assuming that the record pair list is sorted, a greedy assignment algorithm will assign record $A$ to record $B$ (because the weight is larger than for $(A,C)$)\n",
    "- But, then it can not assign record $C$ to record $B$ (even though it is optimal) because record $B$ has already been assigned to $A$.  \n",
    "\n",
    "| * | A | B | C |\n",
    "|-|-|-|-|\n",
    "| <b>A</b> | * | 42.21 | 39.01 | \n",
    "| <b>B</b> | 42.21 | * | 44.98 |\n",
    "| <b>C</b> | 39.01 | 44.98 | * |  \n",
    "\n",
    "- As a result, the assignment would <b><i>not</i> be optimal</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a result, <b>one-to-one assignments must be <i>enforced manually</i></b>, through an additional step in our RL pipeline.\n",
    "- This assignment problem can be solved with special algorithms that find the optimal solution over all possible assignments.\n",
    "    - For example, the [Hungarian algorithm](https://en.wikipedia.org/wiki/Hungarian_algorithm) is a combinatorial optimization algorithm that solves the assignment problem (see next example).\n",
    "- But like many similar algorithms, it does so in polynomial time (slow).\n",
    "- This complexity is acceptable, however, given the reductions in problem size achieved via blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# sample program computing the lowest cost assignment from a cost matrix\n",
    "# https://software.clapper.org/munkres/index.html\n",
    "\n",
    "from munkres import Munkres, print_matrix \n",
    "# install via: $ pip install munkres\n",
    "\n",
    "# matrix representing the costs of each of n workers to perform any of n jobs\n",
    "matrix = [[5, 9, 1],\n",
    "          [10, 3, 2],\n",
    "          [8, 7, 4]]\n",
    "print_matrix(matrix, msg='matrix:')\n",
    "\n",
    "# assignment problem is to assign jobs to workers in a way that minimizes the total cost.\n",
    "# each worker can perform only one job and each job can be assigned to only one worker \n",
    "\n",
    "m = Munkres()\n",
    "indexes = m.compute(matrix)\n",
    "\n",
    "print('lowest costs:')\n",
    "total = 0\n",
    "for row, column in indexes:\n",
    "    value = matrix[row][column]\n",
    "    total += value\n",
    "    print(f'({row}, {column}) -> {value}')\n",
    "print(f'total cost: {total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> After our <b>assignments</b> are complete, we can now begin <font color='#CC3333'><b><i>evaluation</i></b></font> of our results.\n",
    "<center>\n",
    "    <img src=\"assets/evaluation.png\" alt=\"evaluation\" width=\"450\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"evaluation\"></a>\n",
    "<font color='#CC3333'><h2>Evaluation</h2></font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Of course, we still need to determine how successful the linkage algorithm was, in terms of correctly identified pairs.\n",
    "- To do so, we would prefer to have access to <b><i>labeled</i></b> data that can be used for validation.\n",
    "    - This would contain the <i>true match status</i> of all possible matches.\n",
    "- With this, we can precisely evaluate the accuracy of the fitted classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> <b>As always</b>, it is essential that testing data are different than the training data, otherwise <i>over-fitting will occur</i>!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"assets/scale.png\" alt=\"scale\" width=\"150\">\n",
    "</center>\n",
    "\n",
    "In the event we <i>do</i> have labeled data, we must still contend with a <i>formidable <b>class imbalance</b></i>, between \"match\" vs \"non-match\" samples.\n",
    "- Re-sampling is typically not applicable.\n",
    "- As a result, appropriate classification metrics must be applied.\n",
    "    - Accuracy (or misclassification rate), for example, can grow very high by classifying everything as \"non-match\", making it less meaningful for assessing the quality of a set of linked records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Instead, [<i><b>precision</b></i> and <i><b>recall</b></i>](https://en.wikipedia.org/wiki/Precision_and_recall) are used. \n",
    "    - These can be combined into the [<i>$F$-measure</i>](https://en.wikipedia.org/wiki/F1_score), which is the harmonic mean of precision and recall.\n",
    "    - But, the <i>ideal</i> balance between precision and recall will <i>depend on your particular application</i> (medical diagnosis, fraud detection, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Possible matches\n",
    "In this talk, we'll focus on classifying record pairs as either a \"match\" or \"non-match\".\n",
    "- In fact, a third classification can sometimes be useful -- \"possible match\".\n",
    "- These records can then undergo <b>clerical review</b>, and the classified records can then be added to the training data for your model, thus improving your classifier.\n",
    "- This is usually <i>not</i> feasible in most applications, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"demo\"></a>\n",
    "<font color='grey'><h1>Demo</h1></font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For this demo, we'll use the [Python Record Linkage Toolkit](https://recordlinkage.readthedocs.io/en/latest/about.html), a very popular and well-maintained library.\n",
    "- In addition, it comes with a dataset generator, the [Freely Extensible Biomedical Record Linkage](http://users.cecs.anu.edu.au/~Peter.Christen/Febrl/febrl-0.3/febrldoc-0.3/manual.html) (Febrl) package, which we'll use for a brief demo here.\n",
    "- We'll use it for <i>pre-processing and indexing</i>, but bring in other tools for <i>classification and evaluation</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# record linkage toolkit\n",
    "# install via: $ pip install recordlinkage\n",
    "import recordlinkage as rl\n",
    "from recordlinkage.datasets import load_febrl4\n",
    "\n",
    "# metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# load demo data -- simulated biomedical records (labeled 'match' or 'no match')\n",
    "dfA, dfB, true_links = load_febrl4(return_links=True)\n",
    "\n",
    "print(dfA.shape)\n",
    "print(dfB.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# check out dataframe A\n",
    "dfA.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# check out dataframe B\n",
    "dfB.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# indexing step\n",
    "indexer = rl.Index()\n",
    "indexer.block('postcode') # use 'postcode' as blocking key\n",
    "\n",
    "candidate_links = indexer.index(dfA, dfB)\n",
    "\n",
    "print(candidate_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# comparison step\n",
    "compare_cl = rl.Compare()\n",
    "\n",
    "## let's imagine we are working with de-identified data and so no names or birthdates are available\n",
    "# compare_cl.exact('given_name', 'given_name', label='given_name')\n",
    "# compare_cl.string('surname', 'surname', method='jarowinkler', label='surname')\n",
    "# compare_cl.exact('date_of_birth', 'date_of_birth', label='date_of_birth')\n",
    "\n",
    "compare_cl.exact('suburb', 'suburb', label='suburb')\n",
    "compare_cl.exact('state', 'state', label='state')\n",
    "compare_cl.string('address_1', 'address_1', label='address_1')\n",
    "compare_cl.string('address_2', 'address_2', label='address_2')\n",
    "compare_cl.string('address_1', 'address_2', label='address_swapped')\n",
    "\n",
    "# generate feature array\n",
    "features = compare_cl.compute(candidate_links, dfA, dfB)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Let's view an example \"match\" pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# randomly choose a record number\n",
    "record_num = np.random.randint(0, len(true_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# view record from database A\n",
    "dfA[dfA.index==true_links[record_num][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# view record from database B\n",
    "dfB[dfB.index==true_links[record_num][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# view feature vector from record pair\n",
    "features[[ind==true_links[record_num] for ind in features.index.to_list()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"comparing\"></a>\n",
    "## Comparing classifiers\n",
    "\n",
    "`recordlinkage` includes a variety of powerful [classifiers](https://recordlinkage.readthedocs.io/en/latest/ref-classifiers.html#), which we are now free to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But, now that we have our labels and training data, we are free to apply <i>any one</i> of a variety of classification tools available.\n",
    "- So, let's use a handful of standard classification models included in [scikit-learn](https://scikit-learn.org/stable/), which range in complexity.\n",
    "- We could just as easily apply [statsmodels](https://www.statsmodels.org/) or [tensorflow](https://www.tensorflow.org/), instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# convert labels to Booleans to work with sklearn classifiers (inefficient)\n",
    "labels = [element in true_links for element in features.index.to_list()]\n",
    "\n",
    "# Create a training and test set\n",
    "X_train, X_test, y_train, y_test  = train_test_split(features, labels, random_state=0)\n",
    "\n",
    "model_predictions = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"supervised\"></a>\n",
    "### Supervised classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    clf = model.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    print(np.array([['tn', 'fp'], ['fn', 'tp']]))\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print()\n",
    "    print(f\"F1 score: {f1_score(y_test, predictions, average='macro')}\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_predictions['logreg'] = evaluate_model(LogisticRegression(solver='lbfgs', random_state=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model_predictions['nb'] = evaluate_model(MultinomialNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model_predictions['svc'] = evaluate_model(SVC(gamma='auto', random_state=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_predictions['rf'] = evaluate_model(RandomForestClassifier(n_estimators=10, random_state=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Multi-layer Perceptron classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model_predictions['mlp'] = evaluate_model(MLPClassifier(random_state=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"unsupervised\"></a>\n",
    "### Unsupervised classifiers\n",
    "\n",
    "<b>Unfortunately</b>, obtaining ground-truth data is very often not possible and so other solutions must be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model_predictions['kmeans'] = evaluate_model(KMeans(n_clusters=2, random_state=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Expectation/Conditional Maximization Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "model_predictions['ecm'] = evaluate_model(GaussianMixture(n_components=2, random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# compare model performances\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "models = [\n",
    "    'logreg',\n",
    "      'nb',\n",
    "      'svc',\n",
    "      'rf',\n",
    "      'mlp',\n",
    "      'kmeans', \n",
    "      'ecm']\n",
    "\n",
    "# compute ROC curve and ROC area for each model\n",
    "for model in models:\n",
    "    fpr[model], tpr[model], _ = roc_curve(y_test, model_predictions[model])\n",
    "    roc_auc[model] = auc(fpr[model], tpr[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "lw = 2\n",
    "n_classes = len(model_predictions)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "for key in fpr.keys():\n",
    "    plt.plot(fpr[key], tpr[key], lw=lw,\n",
    "             label=f'{key} (auc={roc_auc[key]:0.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "plt.title('receiver operating characteristic')\n",
    "plt.legend(bbox_to_anchor=(1.05, .7))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"conclusions\"></a>\n",
    "<font color='grey'><h1>Conclusions</h1></font>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "1. Records on people or businesses is often very 'messy' with inconsistent formatting from one database to the next.\n",
    "    - This makes finding unique matches unlikely. \n",
    "    - Instead, for each record there is a handful of plausible matches, each matching to a varying degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Typically, pre-processing is necessary in order to normalize undesired variation and ensure consistent formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Since the naive approach has quadratic time complexity, reducing the size of the task becomes essential.\n",
    "    - We do so by beginning with smaller subsets of data that are very likely to contain matching records (i.e. blocking)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "4. Due to the expense of hand-labeling training data, we often need to use an unsupervised approach to RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "5. In spite of our attempts to keep nice, complete databases, people still demand privacy (go figure).\n",
    "    - As a result, fields that include identifying information are often removed or encrypted, adding to the challenge of our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "6. Classification of each record pair into \"matches\" and \"non-matches\" can be performed using either a deterministic (rule-based) or probabilistic (model-based) approach.\n",
    "    - Using a probabilistic approach, comparisons between pairs are broken down into feature vectors summarizing their agreement along multiple dimensions.\n",
    "    - With those feature vectors, we can apply any one of hundreds of different classification models to attempt to predict whether those records are \"matches\" or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"resources\"></a>\n",
    "## Resources\n",
    "- Data Matching by Peter Christen\n",
    "\n",
    "<center>\n",
    "    <a href=\"https://www.springer.com/gp/book/9783642311635\">\n",
    "        <img src=\"https://images.springer.com/sgw/books/medium/9783642311635.jpg\" width=\"200\" alt=\"Data matching book\">\n",
    "    </a>\n",
    "</center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1><i>Thank you,</i></h1>\n",
    "    <img src=\"https://pybay.com/site_media/static/new/img/PyBay2020-Transparent.3c44537b6c56.png\" width=\"200\" alt=\"pybay\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "    <h2><font color='blue'>Big thanks to <i>Grace, Luiz, Karen, et al.</i></font></h2>\n",
    "    <h3><font color='orange'>Another excellent year!</font></h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> ### <i>HMU in [#talk-data-deduplication-with-python](https://pybay2020.slack.com/archives/C018VEY8DKK)!</i>\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
